{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    " \n",
    "transform = transforms.Compose( \n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Normalize((0.5,), (0.5,))]) \n",
    " \n",
    " \n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                               download=True, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, \n",
    "                                          shuffle=True, num_workers=2) \n",
    " \n",
    " \n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                               download=True, transform=transform) \n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, \n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "learning_rate = 0.05\n",
    "# Осуществляем оптимизацию путем стохастического градиентного спуска\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# Создаем функцию потерь\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tigra\\AppData\\Local\\Temp\\ipykernel_14672\\2229801783.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299170\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.360501\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.545165\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.320705\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.231575\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.492642\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.396150\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.182307\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.035727\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.210749\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.219352\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.269956\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.403636\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.292576\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.279745\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.344557\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.198581\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.121551\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.142271\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.107882\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.103787\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.061240\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.116672\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.149826\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.170738\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.162665\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.065326\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.046925\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.112021\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.218259\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# запускаем главный тренировочный цикл\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "# изменим размер с (batch_size, 1, 28, 28) на (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "           print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(trainloader.dataset), 100. * batch_idx / len(trainloader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tigra\\AppData\\Local\\Temp\\ipykernel_14672\\3908785078.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "C:\\Users\\tigra\\AppData\\Local\\Temp\\ipykernel_14672\\2229801783.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test set: Average loss: 0.0000, Accuracy: 61/10000 (1%)\n",
      "1 Test set: Average loss: 0.0000, Accuracy: 124/10000 (1%)\n",
      "2 Test set: Average loss: 0.0000, Accuracy: 186/10000 (2%)\n",
      "3 Test set: Average loss: 0.0000, Accuracy: 248/10000 (2%)\n",
      "4 Test set: Average loss: 0.0000, Accuracy: 310/10000 (3%)\n",
      "5 Test set: Average loss: 0.0000, Accuracy: 369/10000 (4%)\n",
      "6 Test set: Average loss: 0.0000, Accuracy: 428/10000 (4%)\n",
      "7 Test set: Average loss: 0.0000, Accuracy: 489/10000 (5%)\n",
      "8 Test set: Average loss: 0.0000, Accuracy: 553/10000 (6%)\n",
      "9 Test set: Average loss: 0.0000, Accuracy: 614/10000 (6%)\n",
      "10 Test set: Average loss: 0.0000, Accuracy: 675/10000 (7%)\n",
      "11 Test set: Average loss: 0.0000, Accuracy: 734/10000 (7%)\n",
      "12 Test set: Average loss: 0.0000, Accuracy: 796/10000 (8%)\n",
      "13 Test set: Average loss: 0.0000, Accuracy: 857/10000 (9%)\n",
      "14 Test set: Average loss: 0.0000, Accuracy: 915/10000 (9%)\n",
      "15 Test set: Average loss: 0.0000, Accuracy: 976/10000 (10%)\n",
      "16 Test set: Average loss: 0.0000, Accuracy: 1036/10000 (10%)\n",
      "17 Test set: Average loss: 0.0000, Accuracy: 1096/10000 (11%)\n",
      "18 Test set: Average loss: 0.0000, Accuracy: 1157/10000 (12%)\n",
      "19 Test set: Average loss: 0.0000, Accuracy: 1215/10000 (12%)\n",
      "20 Test set: Average loss: 0.0000, Accuracy: 1273/10000 (13%)\n",
      "21 Test set: Average loss: 0.0000, Accuracy: 1335/10000 (13%)\n",
      "22 Test set: Average loss: 0.0000, Accuracy: 1394/10000 (14%)\n",
      "23 Test set: Average loss: 0.0000, Accuracy: 1453/10000 (15%)\n",
      "24 Test set: Average loss: 0.0000, Accuracy: 1511/10000 (15%)\n",
      "25 Test set: Average loss: 0.0000, Accuracy: 1573/10000 (16%)\n",
      "26 Test set: Average loss: 0.0000, Accuracy: 1632/10000 (16%)\n",
      "27 Test set: Average loss: 0.0000, Accuracy: 1690/10000 (17%)\n",
      "28 Test set: Average loss: 0.0000, Accuracy: 1752/10000 (18%)\n",
      "29 Test set: Average loss: 0.0000, Accuracy: 1814/10000 (18%)\n",
      "30 Test set: Average loss: 0.0000, Accuracy: 1872/10000 (19%)\n",
      "31 Test set: Average loss: 0.0000, Accuracy: 1932/10000 (19%)\n",
      "32 Test set: Average loss: 0.0000, Accuracy: 1992/10000 (20%)\n",
      "33 Test set: Average loss: 0.0000, Accuracy: 2051/10000 (21%)\n",
      "34 Test set: Average loss: 0.0000, Accuracy: 2109/10000 (21%)\n",
      "35 Test set: Average loss: 0.0000, Accuracy: 2169/10000 (22%)\n",
      "36 Test set: Average loss: 0.0000, Accuracy: 2230/10000 (22%)\n",
      "37 Test set: Average loss: 0.0000, Accuracy: 2288/10000 (23%)\n",
      "38 Test set: Average loss: 0.0000, Accuracy: 2348/10000 (23%)\n",
      "39 Test set: Average loss: 0.0000, Accuracy: 2404/10000 (24%)\n",
      "40 Test set: Average loss: 0.0000, Accuracy: 2459/10000 (25%)\n",
      "41 Test set: Average loss: 0.0000, Accuracy: 2521/10000 (25%)\n",
      "42 Test set: Average loss: 0.0000, Accuracy: 2581/10000 (26%)\n",
      "43 Test set: Average loss: 0.0000, Accuracy: 2642/10000 (26%)\n",
      "44 Test set: Average loss: 0.0000, Accuracy: 2703/10000 (27%)\n",
      "45 Test set: Average loss: 0.0000, Accuracy: 2762/10000 (28%)\n",
      "46 Test set: Average loss: 0.0000, Accuracy: 2820/10000 (28%)\n",
      "47 Test set: Average loss: 0.0000, Accuracy: 2880/10000 (29%)\n",
      "48 Test set: Average loss: 0.0000, Accuracy: 2939/10000 (29%)\n",
      "49 Test set: Average loss: 0.0000, Accuracy: 3001/10000 (30%)\n",
      "50 Test set: Average loss: 0.0000, Accuracy: 3065/10000 (31%)\n",
      "51 Test set: Average loss: 0.0000, Accuracy: 3126/10000 (31%)\n",
      "52 Test set: Average loss: 0.0000, Accuracy: 3188/10000 (32%)\n",
      "53 Test set: Average loss: 0.0000, Accuracy: 3248/10000 (32%)\n",
      "54 Test set: Average loss: 0.0000, Accuracy: 3309/10000 (33%)\n",
      "55 Test set: Average loss: 0.0000, Accuracy: 3368/10000 (34%)\n",
      "56 Test set: Average loss: 0.0000, Accuracy: 3431/10000 (34%)\n",
      "57 Test set: Average loss: 0.0000, Accuracy: 3491/10000 (35%)\n",
      "58 Test set: Average loss: 0.0000, Accuracy: 3553/10000 (36%)\n",
      "59 Test set: Average loss: 0.0000, Accuracy: 3613/10000 (36%)\n",
      "60 Test set: Average loss: 0.0000, Accuracy: 3670/10000 (37%)\n",
      "61 Test set: Average loss: 0.0000, Accuracy: 3729/10000 (37%)\n",
      "62 Test set: Average loss: 0.0000, Accuracy: 3791/10000 (38%)\n",
      "63 Test set: Average loss: 0.0000, Accuracy: 3848/10000 (38%)\n",
      "64 Test set: Average loss: 0.0000, Accuracy: 3908/10000 (39%)\n",
      "65 Test set: Average loss: 0.0000, Accuracy: 3967/10000 (40%)\n",
      "66 Test set: Average loss: 0.0000, Accuracy: 4025/10000 (40%)\n",
      "67 Test set: Average loss: 0.0000, Accuracy: 4084/10000 (41%)\n",
      "68 Test set: Average loss: 0.0000, Accuracy: 4142/10000 (41%)\n",
      "69 Test set: Average loss: 0.0000, Accuracy: 4203/10000 (42%)\n",
      "70 Test set: Average loss: 0.0000, Accuracy: 4261/10000 (43%)\n",
      "71 Test set: Average loss: 0.0000, Accuracy: 4322/10000 (43%)\n",
      "72 Test set: Average loss: 0.0000, Accuracy: 4382/10000 (44%)\n",
      "73 Test set: Average loss: 0.0000, Accuracy: 4442/10000 (44%)\n",
      "74 Test set: Average loss: 0.0000, Accuracy: 4504/10000 (45%)\n",
      "75 Test set: Average loss: 0.0000, Accuracy: 4562/10000 (46%)\n",
      "76 Test set: Average loss: 0.0000, Accuracy: 4622/10000 (46%)\n",
      "77 Test set: Average loss: 0.0000, Accuracy: 4684/10000 (47%)\n",
      "78 Test set: Average loss: 0.0000, Accuracy: 4748/10000 (47%)\n",
      "79 Test set: Average loss: 0.0000, Accuracy: 4811/10000 (48%)\n",
      "80 Test set: Average loss: 0.0000, Accuracy: 4874/10000 (49%)\n",
      "81 Test set: Average loss: 0.0000, Accuracy: 4937/10000 (49%)\n",
      "82 Test set: Average loss: 0.0000, Accuracy: 5000/10000 (50%)\n",
      "83 Test set: Average loss: 0.0000, Accuracy: 5062/10000 (51%)\n",
      "84 Test set: Average loss: 0.0000, Accuracy: 5126/10000 (51%)\n",
      "85 Test set: Average loss: 0.0000, Accuracy: 5188/10000 (52%)\n",
      "86 Test set: Average loss: 0.0000, Accuracy: 5251/10000 (53%)\n",
      "87 Test set: Average loss: 0.0000, Accuracy: 5315/10000 (53%)\n",
      "88 Test set: Average loss: 0.0000, Accuracy: 5375/10000 (54%)\n",
      "89 Test set: Average loss: 0.0000, Accuracy: 5435/10000 (54%)\n",
      "90 Test set: Average loss: 0.0000, Accuracy: 5498/10000 (55%)\n",
      "91 Test set: Average loss: 0.0000, Accuracy: 5558/10000 (56%)\n",
      "92 Test set: Average loss: 0.0000, Accuracy: 5617/10000 (56%)\n",
      "93 Test set: Average loss: 0.0000, Accuracy: 5675/10000 (57%)\n",
      "94 Test set: Average loss: 0.0000, Accuracy: 5735/10000 (57%)\n",
      "95 Test set: Average loss: 0.0000, Accuracy: 5799/10000 (58%)\n",
      "96 Test set: Average loss: 0.0000, Accuracy: 5862/10000 (59%)\n",
      "97 Test set: Average loss: 0.0000, Accuracy: 5926/10000 (59%)\n",
      "98 Test set: Average loss: 0.0000, Accuracy: 5990/10000 (60%)\n",
      "99 Test set: Average loss: 0.0000, Accuracy: 6053/10000 (61%)\n",
      "100 Test set: Average loss: 0.0000, Accuracy: 6116/10000 (61%)\n",
      "101 Test set: Average loss: 0.0000, Accuracy: 6179/10000 (62%)\n",
      "102 Test set: Average loss: 0.0000, Accuracy: 6239/10000 (62%)\n",
      "103 Test set: Average loss: 0.0000, Accuracy: 6296/10000 (63%)\n",
      "104 Test set: Average loss: 0.0000, Accuracy: 6360/10000 (64%)\n",
      "105 Test set: Average loss: 0.0000, Accuracy: 6422/10000 (64%)\n",
      "106 Test set: Average loss: 0.0000, Accuracy: 6485/10000 (65%)\n",
      "107 Test set: Average loss: 0.0000, Accuracy: 6548/10000 (65%)\n",
      "108 Test set: Average loss: 0.0000, Accuracy: 6609/10000 (66%)\n",
      "109 Test set: Average loss: 0.0000, Accuracy: 6673/10000 (67%)\n",
      "110 Test set: Average loss: 0.0000, Accuracy: 6734/10000 (67%)\n",
      "111 Test set: Average loss: 0.0000, Accuracy: 6797/10000 (68%)\n",
      "112 Test set: Average loss: 0.0000, Accuracy: 6859/10000 (69%)\n",
      "113 Test set: Average loss: 0.0000, Accuracy: 6922/10000 (69%)\n",
      "114 Test set: Average loss: 0.0000, Accuracy: 6986/10000 (70%)\n",
      "115 Test set: Average loss: 0.0000, Accuracy: 7050/10000 (70%)\n",
      "116 Test set: Average loss: 0.0000, Accuracy: 7111/10000 (71%)\n",
      "117 Test set: Average loss: 0.0000, Accuracy: 7172/10000 (72%)\n",
      "118 Test set: Average loss: 0.0000, Accuracy: 7235/10000 (72%)\n",
      "119 Test set: Average loss: 0.0000, Accuracy: 7299/10000 (73%)\n",
      "120 Test set: Average loss: 0.0000, Accuracy: 7362/10000 (74%)\n",
      "121 Test set: Average loss: 0.0000, Accuracy: 7424/10000 (74%)\n",
      "122 Test set: Average loss: 0.0000, Accuracy: 7487/10000 (75%)\n",
      "123 Test set: Average loss: 0.0000, Accuracy: 7547/10000 (75%)\n",
      "124 Test set: Average loss: 0.0000, Accuracy: 7609/10000 (76%)\n",
      "125 Test set: Average loss: 0.0000, Accuracy: 7672/10000 (77%)\n",
      "126 Test set: Average loss: 0.0000, Accuracy: 7734/10000 (77%)\n",
      "127 Test set: Average loss: 0.0000, Accuracy: 7798/10000 (78%)\n",
      "128 Test set: Average loss: 0.0000, Accuracy: 7861/10000 (79%)\n",
      "129 Test set: Average loss: 0.0000, Accuracy: 7924/10000 (79%)\n",
      "130 Test set: Average loss: 0.0000, Accuracy: 7986/10000 (80%)\n",
      "131 Test set: Average loss: 0.0000, Accuracy: 8047/10000 (80%)\n",
      "132 Test set: Average loss: 0.0000, Accuracy: 8110/10000 (81%)\n",
      "133 Test set: Average loss: 0.0000, Accuracy: 8172/10000 (82%)\n",
      "134 Test set: Average loss: 0.0000, Accuracy: 8236/10000 (82%)\n",
      "135 Test set: Average loss: 0.0000, Accuracy: 8300/10000 (83%)\n",
      "136 Test set: Average loss: 0.0000, Accuracy: 8364/10000 (84%)\n",
      "137 Test set: Average loss: 0.0000, Accuracy: 8428/10000 (84%)\n",
      "138 Test set: Average loss: 0.0000, Accuracy: 8492/10000 (85%)\n",
      "139 Test set: Average loss: 0.0000, Accuracy: 8556/10000 (86%)\n",
      "140 Test set: Average loss: 0.0000, Accuracy: 8617/10000 (86%)\n",
      "141 Test set: Average loss: 0.0000, Accuracy: 8678/10000 (87%)\n",
      "142 Test set: Average loss: 0.0000, Accuracy: 8742/10000 (87%)\n",
      "143 Test set: Average loss: 0.0000, Accuracy: 8806/10000 (88%)\n",
      "144 Test set: Average loss: 0.0000, Accuracy: 8870/10000 (89%)\n",
      "145 Test set: Average loss: 0.0000, Accuracy: 8931/10000 (89%)\n",
      "146 Test set: Average loss: 0.0000, Accuracy: 8995/10000 (90%)\n",
      "147 Test set: Average loss: 0.0000, Accuracy: 9057/10000 (91%)\n",
      "148 Test set: Average loss: 0.0000, Accuracy: 9119/10000 (91%)\n",
      "149 Test set: Average loss: 0.0000, Accuracy: 9180/10000 (92%)\n",
      "150 Test set: Average loss: 0.0000, Accuracy: 9241/10000 (92%)\n",
      "151 Test set: Average loss: 0.0000, Accuracy: 9297/10000 (93%)\n",
      "152 Test set: Average loss: 0.0000, Accuracy: 9349/10000 (93%)\n",
      "153 Test set: Average loss: 0.0000, Accuracy: 9411/10000 (94%)\n",
      "154 Test set: Average loss: 0.0000, Accuracy: 9471/10000 (95%)\n",
      "155 Test set: Average loss: 0.0000, Accuracy: 9529/10000 (95%)\n",
      "156 Test set: Average loss: 0.0000, Accuracy: 9545/10000 (95%)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "i = 0\n",
    "for data, target in testloader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    test_loss += criterion(net_out, target)\n",
    "    \n",
    "    pred = net_out.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).sum()\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(testloader.dataset), 100. * correct / len(testloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tigra\\AppData\\Local\\Temp\\ipykernel_14672\\3022312579.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.315754\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.296858\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.226558\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.112155\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.029790\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.580482\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.040598\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.917541\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.825291\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.671217\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.812217\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.527994\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.482099\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.526060\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.623142\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.670540\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.362605\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.534279\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.285516\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.296676\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.463884\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.533344\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.390152\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.307633\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.486737\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.572761\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.253197\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.378407\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.552515\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.361095\n"
     ]
    }
   ],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 100)\n",
    "        self.fc4 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net2 = Net2()\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# запускаем главный тренировочный цикл\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "# изменим размер с (batch_size, 1, 28, 28) на (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net2(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "           print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(trainloader.dataset), 100. * batch_idx / len(trainloader), loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
